---
phase: 01-investigation-instrumentation
plan: 02
type: execute
---

<objective>
Add structured logging and observability to the critical paths identified in Plan 01's DISCOVERY.md.

Purpose: Enable real-time visibility into processing state changes, making it possible to diagnose frozen progress, missing indicators, and silent failures.
Output: Instrumented code paths using the existing RunLogger, with console.log statements replaced by structured logging.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./01-02-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-investigation-instrumentation/DISCOVERY.md

**Existing observability infrastructure:**
@lib/observability/logger.ts
@lib/observability/index.ts

**Files to instrument (based on exploration):**
@server/actions/analysis.ts
@server/actions/batch-stories.ts
@app/api/runs/[id]/route.ts
@app/api/runs/[id]/batch-story/route.ts
@app/api/runs/[id]/process-next/route.ts
@app/api/runs/[id]/process-next-upload/route.ts
@lib/run-engine/process-next-trigger.ts

**Constraints:**
- Use existing RunLogger class - don't create new logging infrastructure
- Replace console.log with structured logger calls where appropriate
- Don't change business logic - only add logging
- Maintain backward compatibility
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Logging to Card Analysis Processing</name>
  <files>app/api/runs/[id]/process-next-upload/route.ts, server/actions/analysis.ts</files>
  <action>
**In server/actions/analysis.ts:**
1. Import `createRunLogger` from `lib/observability`
2. After run creation, log `run.created` with projectId and upload count
3. Log trigger attempt: `invocation.continuation_triggered`
4. Log trigger failure with `continuationFailed` (currently only console.error)

**In app/api/runs/[id]/process-next-upload/route.ts:**
1. Create RunLogger at start of handler: `const logger = createRunLogger(runId, request)`
2. Replace console.log statements with appropriate logger methods:
   - "Processing upload..." → `logger.info("upload.processing", { metadata: { uploadId, filename } })`
   - Status updates → `logger.dbRunUpdated(status, phase)`
   - Heartbeat updates → `logger.dbHeartbeatUpdated()`
   - Errors → `logger.error(...)` with proper error info
3. Log continuation trigger with `continuationTriggered`
4. Log completion/finalization with appropriate run lifecycle events

**Note:** The RunLogger is designed for epic-level context but we'll use it for upload-level context. The methods like `setCurrentEpic` can be repurposed or we can add `setCurrentUpload` if needed - but start with existing methods and metadata fields.
  </action>
  <verify>
grep -n "console.log" app/api/runs/[id]/process-next-upload/route.ts should show fewer hits
grep -n "createRunLogger\|logger\." app/api/runs/[id]/process-next-upload/route.ts shows logger usage
  </verify>
  <done>Card analysis processing has structured logging for key state transitions</done>
</task>

<task type="auto">
  <name>Task 2: Add Logging to Epic Generation Processing</name>
  <files>app/api/runs/[id]/process-next/route.ts, server/actions/batch-stories.ts</files>
  <action>
**In server/actions/batch-stories.ts:**
1. Import `createRunLogger` from `lib/observability`
2. Log run creation with `runCreated` (include projectId, epic count, options)
3. Log trigger attempt and failure (already may have some - verify and enhance)

**In app/api/runs/[id]/process-next/route.ts:**
1. Verify RunLogger is being used (exploration suggests it may already be partially instrumented)
2. Ensure ALL state transitions are logged:
   - `epicStarted` when entering epic processing
   - `claudeRequest` before AI call
   - `claudeResponse` after AI call with duration and tokens
   - `epicCompleted` or `epicFailed` for each epic
   - `runCompleted` or `runFailed` at end
3. Log timeout handling with `invocationTimeoutWarning`
4. Replace any remaining console.log with structured logger

**Key insight from exploration:** Epic generation already uses the loop pattern with heartbeat updates before/after AI calls. Verify these are logged, and add logging for the specific gap: no visibility DURING long AI calls (30-60s).
  </action>
  <verify>
grep -n "logger\." app/api/runs/[id]/process-next/route.ts shows comprehensive logging
grep -n "console.log" app/api/runs/[id]/process-next/route.ts should show few/no hits
  </verify>
  <done>Epic generation processing has structured logging with full visibility into AI call lifecycle</done>
</task>

<task type="auto">
  <name>Task 3: Add Cache Headers and Debug Logging to Progress Endpoints</name>
  <files>app/api/runs/[id]/batch-story/route.ts, app/api/runs/[id]/route.ts</files>
  <action>
**In app/api/runs/[id]/batch-story/route.ts:**
1. Add cache control headers matching card analysis endpoint:
   ```typescript
   return NextResponse.json(progress, {
     headers: {
       "Cache-Control": "no-store, no-cache, must-revalidate",
       "Pragma": "no-cache",
       "Expires": "0",
     }
   });
   ```
2. Add debug logging for polling requests (lightweight, just request received + response status)

**In app/api/runs/[id]/route.ts:**
1. Verify cache headers are present
2. Add currentUpload tracking to response if not present:
   - Find the upload currently being processed (status = LOADING or ANALYZING)
   - Include `currentUploadId`, `currentUploadFilename` in response
3. Add debug logging for polling requests

**Why cache headers matter:** Without them, browsers/CDNs may cache progress responses, causing the UI to show stale data (frozen progress).
  </action>
  <verify>
grep -n "Cache-Control" app/api/runs/[id]/batch-story/route.ts shows header
grep -n "currentUpload" app/api/runs/[id]/route.ts shows tracking
  </verify>
  <done>Progress endpoints return fresh data with no caching, card analysis tracks current upload</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without TypeScript errors
- [ ] No functional changes to processing logic (only logging added)
- [ ] Structured logging present in both processing endpoints
- [ ] Cache headers present on both progress endpoints
- [ ] Current upload tracking in card analysis progress response
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No TypeScript or build errors
- Logging uses existing RunLogger infrastructure
- Ready for Vercel deployment to test in production
</success_criteria>

<output>
After completion, create `.planning/phases/01-investigation-instrumentation/01-02-SUMMARY.md`:

# Phase 1 Plan 2: Add Instrumentation Summary

**[Key instrumentation added - where and why]**

## Accomplishments

- [Logging added to X]
- [Cache headers fixed]
- [Current upload tracking added]

## Files Created/Modified

- `path/to/file.ts` - What changed

## Decisions Made

[Any choices made during implementation]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Phase 1 complete. Next: Phase 2 (Card Analysis Progress Fix)

**Verification needed:** Deploy to Vercel and test:
1. Run card analysis - check Vercel logs for structured events
2. Run epic generation - check logs show progress through AI calls
3. Verify progress endpoints return fresh data (check response headers)
</output>
